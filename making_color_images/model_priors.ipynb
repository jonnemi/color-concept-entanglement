{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadde00",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (701316962.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom ../test_MLLMs.ipynb import mllm_testing\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "from PIL import Image\n",
    "import pandas as pd, torch, gc\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "import nbimporter\n",
    "from test_MLLMs.ipynb import mllm_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab45854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ground truth colors are influenced differ if dummy images are used\n",
    "def get_model_color_priors(df, processor, model):\n",
    "    batch_size = 1\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Running predictions\"):\n",
    "        batch_df = df.iloc[i:i + batch_size].copy()\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            df_dummy = mllm_testing(batch_df, processor, model, most=\"True\", dummy_image=True)\n",
    "            df_dummy = df_dummy.rename(columns={\"predicted_color\": \"model_prior_dummy\"})\n",
    "\n",
    "            df_real = mllm_testing(batch_df, processor, model, most=\"True\", dummy_image=False)\n",
    "            df_real = df_real.rename(columns={\"predicted_color\": \"model_prior\"})\n",
    "\n",
    "            df_merged = pd.concat([df_dummy, df_real[[\"model_prior\"]]], axis=1)\n",
    "\n",
    "        results.append(df_merged)\n",
    "        del df_merged\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "\n",
    "    ground_truth_df = pd.concat(results, ignore_index=True)\n",
    "    out_path = data_folder / \"model_color_priors.csv\"\n",
    "    ground_truth_df.to_csv(out_path, index=False)\n",
    "    display(ground_truth_df[['object', 'correct_answer', 'model_prior_dummy', 'model_prior']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = pd.read_csv(data_folder / \"model_color_priors.csv\")\n",
    "display(ground_truth_df[['object', 'correct_answer', 'model_prior_dummy', 'model_prior']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5045023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df[\"diff_dummy_vs_real\"] = ground_truth_df[\"model_prior_dummy\"].str.lower() != ground_truth_df[\"model_prior\"].str.lower()\n",
    "print(f\"There are {ground_truth_df[ground_truth_df['diff_dummy_vs_real']].shape[0]} rows where the predicted color differs between dummy and real images.\")\n",
    "\n",
    "ground_truth_df[\"correct_answer\"] = ground_truth_df[\"correct_answer\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "ground_truth_df[\"color_in_gt\"] = ground_truth_df.apply(\n",
    "    lambda r: r[\"model_prior\"].lower() in [c.lower() for c in r[\"correct_answer\"]],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where model specific priors differ from correct answer from Visual Counterfact\n",
    "print(f\"There are {ground_truth_df[~ground_truth_df['color_in_gt']].shape[0]} rows where the model color prior is NOT in the ground truth answers from Visual Counterfact.\")\n",
    "ground_truth_df[~ground_truth_df[\"color_in_gt\"]][[\"object\", \"correct_answer\", \"model_prior\"]]\n",
    "\n",
    "model_priors = ground_truth_df[\"model_prior\"].unique()\n",
    "print(f\"Model color priors: {model_priors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5091569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with silver, gold, white clear ground truth answers\n",
    "colors_to_exclude = [\"silver\", \"gold\", \"white\", \"clear\"]\n",
    "ground_truth_df = ground_truth_df[\n",
    "    ~ground_truth_df[\"model_prior\"].isin(colors_to_exclude)\n",
    "]\n",
    "print(f\"We are left with {ground_truth_df.shape[0]} rows after excluding colors: {colors_to_exclude}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f343e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Visial Counterfact correct answers with model specific priors\n",
    "df = df.merge(\n",
    "    ground_truth_df[[\"object\", \"model_prior\"]],\n",
    "    on=\"object\",\n",
    "    how=\"inner\"  # <-- ensures only shared objects remain\n",
    ")\n",
    "df[\"correct_answer\"] = df[\"model_prior\"]\n",
    "df = df.drop(columns=[\"model_prior\"])\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
