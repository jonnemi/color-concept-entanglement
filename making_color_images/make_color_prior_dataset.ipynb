{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44093122",
   "metadata": {},
   "source": [
    "# Make color-prior object dataset\n",
    "This notebook creates a dataset of real-world objects with **known color priors**.\n",
    "Images are obtained through targeted queries in *query_google_images.ipynb* and segmented to produce binary foreground masks using *segment_outline.ipynb*.\n",
    "Using these masks, foreground and background regions are colored independently, enabling controlled manipulations of object color while preserving object identity.\n",
    "Recoloring is implemented via *recolor_images.py*.\n",
    "Color priors are model-specific and can be queried using *model_priors.py*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f815b0",
   "metadata": {},
   "source": [
    "## 1. Pipeline for LLaVA-NeXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0593c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlavaNextProcessor,\n",
    "    LlavaNextForConditionalGeneration\n",
    ")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from test_MLLMs import run_vlm_evaluation\n",
    "from making_color_images.model_priors import TorchColorPriors, GPTColorPriors\n",
    "from making_color_images.plot_variants import collect_variants_for, show_variants_grid, plot_vlm_performance, variant_label\n",
    "from making_color_images.recolor_images import generate_variants, resize_all_images_and_masks\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "fontsize = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a specific seed for reproducibility\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# Setting the seed for PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # If using GPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model_name = \"llava-v1.6-mistral-7b-hf\"\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\", dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\", quantization_config=bnb_config\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8137cf",
   "metadata": {},
   "source": [
    "## 1. Load segmented images and query world-knowledge priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107da69c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load dataframe with outline images and masks\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(DATA / \u001b[33m\"\u001b[39m\u001b[33msegmented_images.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m display(df)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load dataframe with outline images and masks\n",
    "df = pd.read_csv(DATA / \"segmented_images.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize ModelColorPriors\n",
    "priors = TorchColorPriors(\n",
    "    processor=processor,\n",
    "    model=model,\n",
    "    data_folder=DATA,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadf445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new priors\n",
    "# priors_df = priors.get_model_color_priors(df, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd878bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: display the generated priors\n",
    "priors_df = priors.load_model_priors()\n",
    "display(priors_df[['object', 'correct_answer', 'dummy_priors', 'image_priors']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d8df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
