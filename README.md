# Probing Color–Concept Entanglement in Vision–Language Models

This project investigates how vision–language models (VLMs) use color cues to recognize and associate object concepts.
It explores whether color serves as a shortcut or confound in multimodal inference through two experiments:

Pixel Injection: Measuring how few pixels of a diagnostic color (e.g., red for “strawberry”) are needed for a model to make a confident prediction.

Color–Word Incongruence (Stroop Test): Testing model behavior when color and textual labels conflict.

The goal is to better understand how color priors influence concept activation and to shed light on emergent color semantics in multimodal models.
