{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e16b11",
   "metadata": {},
   "source": [
    "# Vision Language Model Evaluation\n",
    "\n",
    "This notebook evaluates multiple vision language models (e.g., LLaVA-NeXT, GPT-5) on three systematically constructed image datasets designed to probe color-concept representations: canonical object colors, counterfactual recolorings and abstract colored shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d06ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlavaNextProcessor,\n",
    "    LlavaNextForConditionalGeneration\n",
    ")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT = Path().resolve().parents[0]\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from test_MLLMs import run_vlm_evaluation\n",
    "from making_color_images.model_priors import TorchColorPriors, GPTColorPriors\n",
    "from making_color_images.plot_variants import collect_variants_for, show_variants_grid, plot_vlm_performance, variant_label\n",
    "from making_color_images.recolor_images import generate_variants, resize_all_images_and_masks\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "fontsize = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8d2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base data folder\n",
    "WORK = Path(os.environ.get(\"WORK\", Path.cwd()))\n",
    "DATA = WORK / \"color-concept-entanglement\" / \"data\"\n",
    "\n",
    "FRUIT = DATA / \"fruit\"\n",
    "OUTLINES = DATA / \"color_images\"\n",
    "\n",
    "RESIZED_IMGS = DATA / \"resized_images\"\n",
    "RESIZED_MASKS = DATA / \"resized_cv_masks\"\n",
    "RESIZED_IMGS.mkdir(parents=True, exist_ok=True)\n",
    "RESIZED_MASKS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566219bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1013"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a specific seed for reproducibility\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# Setting the seed for PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # If using GPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6a32f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>object</th>\n",
       "      <th>stimulus_type</th>\n",
       "      <th>manipulation_color</th>\n",
       "      <th>target_color</th>\n",
       "      <th>variant_region</th>\n",
       "      <th>percent_colored</th>\n",
       "      <th>variant_label</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>color_images/gpt-4o/image_priors/cheese_1_78f6...</td>\n",
       "      <td>cheese</td>\n",
       "      <td>correct_prior</td>\n",
       "      <td>yellow</td>\n",
       "      <td>yellow</td>\n",
       "      <td>BG</td>\n",
       "      <td>80</td>\n",
       "      <td>BG 80% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>color_images/gpt-4o/image_priors/espresso_make...</td>\n",
       "      <td>espresso maker</td>\n",
       "      <td>correct_prior</td>\n",
       "      <td>red</td>\n",
       "      <td>red</td>\n",
       "      <td>FG</td>\n",
       "      <td>5</td>\n",
       "      <td>FG 5% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>color_images/gpt-4o/image_priors/tile_roof_2_f...</td>\n",
       "      <td>tile roof</td>\n",
       "      <td>correct_prior</td>\n",
       "      <td>red</td>\n",
       "      <td>red</td>\n",
       "      <td>FG</td>\n",
       "      <td>100</td>\n",
       "      <td>FG 100% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>color_images/gpt-4o/image_priors/cloud_3_29898...</td>\n",
       "      <td>cloud</td>\n",
       "      <td>correct_prior</td>\n",
       "      <td>grey</td>\n",
       "      <td>grey</td>\n",
       "      <td>FG</td>\n",
       "      <td>55</td>\n",
       "      <td>FG 55% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>color_images/gpt-4o/image_priors/frilled_lizar...</td>\n",
       "      <td>frilled lizard</td>\n",
       "      <td>correct_prior</td>\n",
       "      <td>brown</td>\n",
       "      <td>brown</td>\n",
       "      <td>FG</td>\n",
       "      <td>100</td>\n",
       "      <td>FG 100% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path          object  \\\n",
       "0  color_images/gpt-4o/image_priors/cheese_1_78f6...          cheese   \n",
       "1  color_images/gpt-4o/image_priors/espresso_make...  espresso maker   \n",
       "2  color_images/gpt-4o/image_priors/tile_roof_2_f...       tile roof   \n",
       "3  color_images/gpt-4o/image_priors/cloud_3_29898...           cloud   \n",
       "4  color_images/gpt-4o/image_priors/frilled_lizar...  frilled lizard   \n",
       "\n",
       "   stimulus_type manipulation_color target_color variant_region  \\\n",
       "0  correct_prior             yellow       yellow             BG   \n",
       "1  correct_prior                red          red             FG   \n",
       "2  correct_prior                red          red             FG   \n",
       "3  correct_prior               grey         grey             FG   \n",
       "4  correct_prior              brown        brown             FG   \n",
       "\n",
       "   percent_colored  variant_label mode  \n",
       "0               80   BG 80% (seq)  seq  \n",
       "1                5    FG 5% (seq)  seq  \n",
       "2              100  FG 100% (seq)  seq  \n",
       "3               55   FG 55% (seq)  seq  \n",
       "4              100  FG 100% (seq)  seq  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1260, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>object</th>\n",
       "      <th>stimulus_type</th>\n",
       "      <th>manipulation_color</th>\n",
       "      <th>target_color</th>\n",
       "      <th>variant_region</th>\n",
       "      <th>percent_colored</th>\n",
       "      <th>variant_label</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>color_images/gpt-4o/counterfact/rose_3_6471302...</td>\n",
       "      <td>rose</td>\n",
       "      <td>counterfact</td>\n",
       "      <td>blue</td>\n",
       "      <td>blue</td>\n",
       "      <td>FG</td>\n",
       "      <td>100</td>\n",
       "      <td>FG 100% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>color_images/gpt-4o/counterfact/Sealyham_terri...</td>\n",
       "      <td>sealyham terrier</td>\n",
       "      <td>counterfact</td>\n",
       "      <td>purple</td>\n",
       "      <td>purple</td>\n",
       "      <td>FG</td>\n",
       "      <td>60</td>\n",
       "      <td>FG 60% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>color_images/gpt-4o/counterfact/iguana_2_a2663...</td>\n",
       "      <td>iguana</td>\n",
       "      <td>counterfact</td>\n",
       "      <td>orange</td>\n",
       "      <td>orange</td>\n",
       "      <td>FG</td>\n",
       "      <td>10</td>\n",
       "      <td>FG 10% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>color_images/gpt-4o/counterfact/hartebeest_3_5...</td>\n",
       "      <td>hartebeest</td>\n",
       "      <td>counterfact</td>\n",
       "      <td>red</td>\n",
       "      <td>red</td>\n",
       "      <td>FG</td>\n",
       "      <td>55</td>\n",
       "      <td>FG 55% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>color_images/gpt-4o/counterfact/mouse_2_cf4ddb...</td>\n",
       "      <td>mouse</td>\n",
       "      <td>counterfact</td>\n",
       "      <td>red</td>\n",
       "      <td>red</td>\n",
       "      <td>FG</td>\n",
       "      <td>20</td>\n",
       "      <td>FG 20% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path            object  \\\n",
       "0  color_images/gpt-4o/counterfact/rose_3_6471302...              rose   \n",
       "1  color_images/gpt-4o/counterfact/Sealyham_terri...  sealyham terrier   \n",
       "2  color_images/gpt-4o/counterfact/iguana_2_a2663...            iguana   \n",
       "3  color_images/gpt-4o/counterfact/hartebeest_3_5...        hartebeest   \n",
       "4  color_images/gpt-4o/counterfact/mouse_2_cf4ddb...             mouse   \n",
       "\n",
       "  stimulus_type manipulation_color target_color variant_region  \\\n",
       "0   counterfact               blue         blue             FG   \n",
       "1   counterfact             purple       purple             FG   \n",
       "2   counterfact             orange       orange             FG   \n",
       "3   counterfact                red          red             FG   \n",
       "4   counterfact                red          red             FG   \n",
       "\n",
       "   percent_colored  variant_label mode  \n",
       "0              100  FG 100% (seq)  seq  \n",
       "1               60   FG 60% (seq)  seq  \n",
       "2               10   FG 10% (seq)  seq  \n",
       "3               55   FG 55% (seq)  seq  \n",
       "4               20   FG 20% (seq)  seq  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(412, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>object</th>\n",
       "      <th>stimulus_type</th>\n",
       "      <th>manipulation_color</th>\n",
       "      <th>target_color</th>\n",
       "      <th>variant_region</th>\n",
       "      <th>percent_colored</th>\n",
       "      <th>variant_label</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shapes/shape_colored/hexagon_v3_yellow/FG_060_...</td>\n",
       "      <td>hexagon</td>\n",
       "      <td>shape</td>\n",
       "      <td>yellow</td>\n",
       "      <td>yellow</td>\n",
       "      <td>FG</td>\n",
       "      <td>60</td>\n",
       "      <td>FG 60% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shapes/shape_colored/pentagon_v0_purple/FG_055...</td>\n",
       "      <td>pentagon</td>\n",
       "      <td>shape</td>\n",
       "      <td>purple</td>\n",
       "      <td>purple</td>\n",
       "      <td>FG</td>\n",
       "      <td>55</td>\n",
       "      <td>FG 55% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shapes/shape_colored/square_v3_blue/FG_005_seq...</td>\n",
       "      <td>square</td>\n",
       "      <td>shape</td>\n",
       "      <td>blue</td>\n",
       "      <td>blue</td>\n",
       "      <td>FG</td>\n",
       "      <td>5</td>\n",
       "      <td>FG 5% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shapes/shape_colored/triangle_v0_brown/FG_050_...</td>\n",
       "      <td>triangle</td>\n",
       "      <td>shape</td>\n",
       "      <td>brown</td>\n",
       "      <td>brown</td>\n",
       "      <td>FG</td>\n",
       "      <td>50</td>\n",
       "      <td>FG 50% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shapes/shape_colored/pentagon_v2_orange/FG_090...</td>\n",
       "      <td>pentagon</td>\n",
       "      <td>shape</td>\n",
       "      <td>orange</td>\n",
       "      <td>orange</td>\n",
       "      <td>FG</td>\n",
       "      <td>90</td>\n",
       "      <td>FG 90% (seq)</td>\n",
       "      <td>seq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path    object stimulus_type  \\\n",
       "0  shapes/shape_colored/hexagon_v3_yellow/FG_060_...   hexagon         shape   \n",
       "1  shapes/shape_colored/pentagon_v0_purple/FG_055...  pentagon         shape   \n",
       "2  shapes/shape_colored/square_v3_blue/FG_005_seq...    square         shape   \n",
       "3  shapes/shape_colored/triangle_v0_brown/FG_050_...  triangle         shape   \n",
       "4  shapes/shape_colored/pentagon_v2_orange/FG_090...  pentagon         shape   \n",
       "\n",
       "  manipulation_color target_color variant_region  percent_colored  \\\n",
       "0             yellow       yellow             FG               60   \n",
       "1             purple       purple             FG               55   \n",
       "2               blue         blue             FG                5   \n",
       "3              brown        brown             FG               50   \n",
       "4             orange       orange             FG               90   \n",
       "\n",
       "  variant_label mode  \n",
       "0  FG 60% (seq)  seq  \n",
       "1  FG 55% (seq)  seq  \n",
       "2   FG 5% (seq)  seq  \n",
       "3  FG 50% (seq)  seq  \n",
       "4  FG 90% (seq)  seq  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1331, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the datasets that where also used in the human evaluation\n",
    "prior_df = pd.read_csv(DATA / \"prolific_stimuli\" / f\"stimulus_table_image_priors_prolific.csv\")\n",
    "counterfact_df = pd.read_csv(DATA / \"prolific_stimuli\" / f\"stimulus_table_counterfact_prolific.csv\")\n",
    "shape_df = pd.read_csv(DATA / \"prolific_stimuli\" / f\"stimulus_table_shapes_prolific.csv\")\n",
    "display(prior_df.head(), prior_df.shape, counterfact_df.head(), counterfact_df.shape, shape_df.head(), shape_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b633f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vlm_on_stimuli(\n",
    "    df_stimuli: pd.DataFrame,\n",
    "    *,\n",
    "    processor,\n",
    "    model,\n",
    "    device,\n",
    "    image_root: Path,\n",
    "    batch_size: int = 1,\n",
    "    mode: str = \"this\",\n",
    "    desc: str = \"Evaluating stimuli\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a VLM on a set of stimulus images.\n",
    "\n",
    "    Expects df_stimuli to have columns:\n",
    "      - image_path (relative to image_root)\n",
    "      - object\n",
    "      - target_color\n",
    "    \"\"\"\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for _, row in tqdm(\n",
    "        df_stimuli.iterrows(),\n",
    "        total=len(df_stimuli),\n",
    "        desc=desc,\n",
    "    ):\n",
    "\n",
    "        df_eval_input = pd.DataFrame({\n",
    "            \"image_path\": [image_root / row[\"image_path\"]],\n",
    "            \"object\": [row[\"object\"]],\n",
    "            \"correct_answer\": [row[\"target_color\"]],\n",
    "        })\n",
    "\n",
    "        df_eval = run_vlm_evaluation(\n",
    "            df=df_eval_input,\n",
    "            processor=processor,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "            mode=mode,\n",
    "            return_probs=True,\n",
    "        )\n",
    "\n",
    "        preds.append(\n",
    "            df_eval[[\n",
    "                \"object\",\n",
    "                \"image_path\",\n",
    "                \"correct_answer\",\n",
    "                \"pred_color_this\",\n",
    "                \"prob_correct_this\",\n",
    "            ]]\n",
    "        )\n",
    "\n",
    "        # GPU hygiene (important for long runs)\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "\n",
    "    return pd.concat(preds, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef688195",
   "metadata": {},
   "source": [
    "# 1. LLaVA-NeXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1ad85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5644d8312734ba6916ddb392106c3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"llava-v1.6-mistral-7b-hf\"\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\", dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\", quantization_config=bnb_config\n",
    ").to(device)\n",
    "\n",
    "LLAVA = DATA / \"LLaVA-NeXT_results\"\n",
    "LLAVA.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f846e",
   "metadata": {},
   "source": [
    "## 1.1. Color Prior Dataset (LLaVA-NeXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00837b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating canonical object colors (correct-prior stimuli):   0%|          | 4/1260 [00:10<54:45,  2.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_priors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m shape_pred_df \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_vlm_on_stimuli\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprior_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluating canonical object colors (correct-prior stimuli)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m out_path \u001b[38;5;241m=\u001b[39m LLAVA \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_shapes_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m shape_pred_df\u001b[38;5;241m.\u001b[39mto_csv(out_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m, in \u001b[0;36mevaluate_vlm_on_stimuli\u001b[0;34m(df_stimuli, processor, model, device, image_root, batch_size, mode, desc)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     24\u001b[0m     df_stimuli\u001b[38;5;241m.\u001b[39miterrows(),\n\u001b[1;32m     25\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df_stimuli),\n\u001b[1;32m     26\u001b[0m     desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m     27\u001b[0m ):\n\u001b[1;32m     29\u001b[0m     df_eval_input \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: [image_root \u001b[38;5;241m/\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m: [row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: [row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_color\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     33\u001b[0m     })\n\u001b[0;32m---> 35\u001b[0m     df_eval \u001b[38;5;241m=\u001b[39m \u001b[43mrun_vlm_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_eval_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     preds\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     46\u001b[0m         df_eval[[\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m         ]]\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# GPU hygiene (important for long runs)\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/lustre/work/eickhoff/esx061/color-concept-entanglement/model_evaluation/test_MLLMs.py:337\u001b[0m, in \u001b[0;36mrun_vlm_evaluation\u001b[0;34m(df, processor, model, device, batch_size, mode, dummy, return_probs, use_gpt)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    336\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m create_eval_prompt(batch_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m], most\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 337\u001b[0m     df_this \u001b[38;5;241m=\u001b[39m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     df_this \u001b[38;5;241m=\u001b[39m df_this\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_color\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_color_this\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob_correct\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob_correct_this\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_probs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     })\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/lustre/work/eickhoff/esx061/color-concept-entanglement/model_evaluation/test_MLLMs.py:316\u001b[0m, in \u001b[0;36mrun_vlm_evaluation.<locals>.<lambda>\u001b[0;34m(batch, prompt)\u001b[0m\n\u001b[1;32m    311\u001b[0m     caller \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m batch, prompt: prompt_gpt_sync(\n\u001b[1;32m    312\u001b[0m         batch, prompt, model_name\u001b[38;5;241m=\u001b[39mgpt_model_name, dummy\u001b[38;5;241m=\u001b[39mdummy\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     caller \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m batch, prompt: \u001b[43mprompt_mllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_probs\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df), batch_size), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning VLM (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39muse_gpt\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTorch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    322\u001b[0m     batch_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i : i \u001b[38;5;241m+\u001b[39m batch_size]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/mnt/lustre/work/eickhoff/esx061/color-concept-entanglement/model_evaluation/test_MLLMs.py:107\u001b[0m, in \u001b[0;36mprompt_mllm\u001b[0;34m(df, processor, model, device, prompt, dummy, return_probs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m inputs, outputs\n\u001b[1;32m    106\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 107\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    109\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_color\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m generated_texts\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_probs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "suffix = \"image_priors\"\n",
    "shape_pred_df = evaluate_vlm_on_stimuli(\n",
    "    prior_df,\n",
    "    processor=processor,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    image_root=DATA,\n",
    "    desc=\"Evaluating canonical object colors (correct-prior stimuli)\",\n",
    ")\n",
    "\n",
    "out_path = LLAVA / f\"evaluation_shapes_{model_name}_{suffix}.csv\"\n",
    "shape_pred_df.to_csv(out_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d3da4",
   "metadata": {},
   "source": [
    "## 1.2. Counterfact Color Dataset (LLaVA-NeXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"counterfact\"\n",
    "shape_pred_df = evaluate_vlm_on_stimuli(\n",
    "    counterfact_df,\n",
    "    processor=processor,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    image_root=DATA,\n",
    "    desc=\"Evaluating counterfact object colors (counterfact stimuli)\",\n",
    ")\n",
    "\n",
    "out_path = LLAVA / f\"evaluation_shapes_{model_name}_{suffix}.csv\"\n",
    "shape_pred_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6946af",
   "metadata": {},
   "source": [
    "## 1.3. Shape Dataset (LLaVA-NeXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a399d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"shapes\"\n",
    "shape_pred_df = evaluate_vlm_on_stimuli(\n",
    "    shape_df,\n",
    "    processor=processor,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    image_root=DATA,\n",
    "    desc=\"Evaluating shapes\",\n",
    ")\n",
    "\n",
    "out_path = LLAVA / f\"evaluation_shapes_{model_name}_{suffix}.csv\"\n",
    "shape_pred_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d1974",
   "metadata": {},
   "source": [
    "# 2. GPT-5o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5569e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-5o\"\n",
    "GPT = DATA / \"GPT_results\"\n",
    "GPT.mkdir(parents=True, exist_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
